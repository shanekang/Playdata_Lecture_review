{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1f4V54IrEFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "87c20da8-bae4-43d6-c280-fedfd55aa597"
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 픽셀 값을 0~1 사이로 정규화합니다.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo9ym2YzrPHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRW3Emc2rP9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a284f41a-edb1-46d2-fa0d-638c2aaf581c"
      },
      "source": [
        "plt.imshow(x_train[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f845f11e6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaXVwqRxr4j4",
        "colab_type": "text"
      },
      "source": [
        "1. 데이터 로딩 \n",
        "2. 하이퍼파라미터 설정하기 \n",
        "3. 신경망 & 출력층 설계 및 구현\n",
        "  * 2층 짜리 신경망\n",
        "4. 미니배치 구성 & 학습\n",
        "5. 기울기 (gradient) 계산\n",
        "6. 가중치 업데이트 \n",
        "7. 학습 경과 기록하기 (loss function)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ86uIzYsq8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_flatten = x_train.reshape([x_train.shape[0], -1]) \n",
        "x_test_flatten = x_test.reshape([x_test.shape[0], -1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJXad3zcIeA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6975682-a58d-410f-aa0e-147ad9c567d0"
      },
      "source": [
        "# t_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8xYwdi6r3nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 하이퍼파라미터 설정 \n",
        "iters_num = 10000 # 반복 횟수\n",
        "train_size = x_train_flatten. shape[0] # 28x28 -> 784로 flatten\n",
        "batch_size = 100 # 미니배치 크기 \n",
        "learning_rate = 0.1 # 학습률 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72MwlhFQDPYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(a):\n",
        "  ## a 배열 각 요소에 exp 실행 \n",
        "  exp_a = np.exp(a)\n",
        "  ## 분모에 들어갈 합 계산 \n",
        "  sum_exp_a = np.sum(exp_a)\n",
        "  ## 각 요소를 합으로 나눠줌 \n",
        "  y = exp_a / sum_exp_a\n",
        "\n",
        "  return y\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdvEbhgpDTcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size) \n",
        "  \n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAngcg-PrSnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(network, x):\n",
        "  W1, W2 = network['W1'], network['W2']\n",
        "  b1, b2 = network['b1'], network['b2']\n",
        "\n",
        "  # 입력층 -> 은닉층 (1층)\n",
        "  a1 = np.dot(x, W1) + b1\n",
        "  z1 = sigmoid(a1)\n",
        "\n",
        "  # 1층 -> 2층 \n",
        "  a2 = np.dot(z1, W2) + b2 \n",
        "  # MNIST data를 분류하는 문제니까.. \n",
        "  # 마지막 layer의 활성화 함수는 => softmax\n",
        "  y = softmax(a2) \n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nqcEsNZxNvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZtAfwGb3Z15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x) \n",
        "\n",
        "  for idx in range(x.shape[0]):\n",
        "    tmp_val = x[idx]\n",
        "\n",
        "    # y증가량 / x증가량 = 기울기 \n",
        "    # y증가량 : f(x+h) - f(x-h)\n",
        "    # f(x+h) \n",
        "    x[idx] = tmp_val + h \n",
        "    fxh1 = f(x)\n",
        "\n",
        "    # f(x-h)\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    # gradient 값 저장 \n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    x[idx] = tmp_val\n",
        "\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBhI0AGp8-7G",
        "colab_type": "text"
      },
      "source": [
        "## 2층 신경망 class 정의 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z14Q2jyi8-rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    # 가중치 초기화 \n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1 \n",
        "    z1 = sigmoid(a1)\n",
        "\n",
        "    a2 = np.dot(z1, W2) + b2 \n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y \n",
        "\n",
        "  # 손실 함수 값 x : 입력 데이터, t : 정답 데이터 \n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "    return loss\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y==t) / float(x.shape[0])\n",
        "\n",
        "    return accuracy \n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    \n",
        "    return grads "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Menu3Nd4vI01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 하이퍼파라미터 설정 \n",
        "iters_num = 10000 # 반복 횟수\n",
        "train_size = x_train_flatten.shape[0] # 28x28 -> 784로 flatten\n",
        "batch_size = 100 # 미니배치 크기 \n",
        "learning_rate = 0.01 # 학습률 \n",
        "\n",
        "# 네트워크 \n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 학습할때는 'batch_size' 만큼씩 뽑아서 사용 \n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  # batch input\n",
        "  x_batch = x_train_flatten[batch_mask]\n",
        "  # 정답 label\n",
        "  t_batch = t_train[batch_mask]\n",
        "  \n",
        "  # 기울기 (gradient) 계산\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "  # 가중치 업데이트\n",
        "  for key in ('W1', 'W2', 'b1', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록하기 (loss function)\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "  print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s65isohfGW69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}